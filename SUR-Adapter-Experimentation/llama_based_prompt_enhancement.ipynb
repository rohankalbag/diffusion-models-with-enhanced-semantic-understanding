{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f88d282f-92dd-4383-bf55-47bab7eaa21f",
   "metadata": {},
   "source": [
    "## Code Exploring LLAMA based Prompt Enhancement for SUR-Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc7d1278-6361-4a3d-a7e0-1d3f1ce0b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3ec79f0-4ff7-4c54-a99f-6459dde0fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption_detailing_prompt(caption):\n",
    "    base_prompt = '''\n",
    "    Please generate the long prompt version of the short one according to the given examples. Long prompt version should consist of 3 to 5 sentences. Long prompt version must specify the color, shape, texture or spatial relation of the included objects. DO NOT generate sentences that describe any atmosphere!!!\n",
    "    \n",
    "        Short: A calico cat with eyes closed is perched upon a Mercedes.\n",
    "        Long: a multicolored cat perched atop a shiny black car. the car is parked in front of a building with wooden walls and a green fence. the reflection of the car and the surrounding environment can be seen on the car's glossy surface.\n",
    "    \n",
    "        Short: A boys sitting on a chair holding a video game remote.\n",
    "        Long: a young boy sitting on a chair, wearing a blue shirt and a baseball cap with the letter 'm'. he has a red medal around his neck and is holding a white game controller. behind him, there are two other individuals, one of whom is wearing a backpack. to the right of the boy, there's a blue trash bin with a sign that reads 'automatic party'.\n",
    "    \n",
    "        Short: A man is on the bank of the water fishing.\n",
    "        Long: a serene waterscape where a person, dressed in a blue jacket and a red beanie, stands in shallow waters, fishing with a long rod. the calm waters are dotted with several sailboats anchored at a distance, and a mountain range can be seen in the background under a cloudy sky.\n",
    "    \n",
    "        Short: A kitchen with a cluttered counter and wooden cabinets.\n",
    "        Long: a well-lit kitchen with wooden cabinets, a black and white checkered floor, and a refrigerator adorned with a floral decal on its side. the kitchen countertop holds various items, including a coffee maker, jars, and fruits.\n",
    "    \n",
    "        Short: \n",
    "        %s\n",
    "    Strictly return the output as following \"ANSWER: {long prompt for above short}\", do not give any other text as output\n",
    "    '''\n",
    "    return base_prompt % caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a16f0c52-73fb-4d43-a233-3ceb0e551080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0038175582885742188,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71ff835de6d4e6e8bd9ca5ef92f62ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "import accelerate\n",
    "\n",
    "device = torch.device('cuda:4')\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "pipeline=transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    return_full_text=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3807b2f2-de5b-4c69-a6fe-730becc7a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_detailed_caption_with_llama(caption):\n",
    "    t1 = time.perf_counter()\n",
    "    sequences = pipeline(get_caption_detailing_prompt(caption))\n",
    "    print(\"llama inference took:\", time.perf_counter() - t1)\n",
    "    return sequences[0]['generated_text'].strip(\" \\nLong:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be2247b1-d7a2-4378-987c-08f86f91c8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama inference took: 3.444841541000642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a majestic elephant with a wrinkled gray hide, standing near a lush green tree. the elephant is holding an apple in its long, flexible trunk, and its tusks glint in the sunlight. in the background, a group of birds can be seen flying overhead, and a distant mountain range can be seen through the trees.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_detailed_caption_with_llama(\"An elephant holding an apple in its trunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0bbb1928-b2ee-4845-88ee-d13e550203fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "diff_model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    diff_model_name,\n",
    "    subfolder=\"tokenizer\",\n",
    "    revision=None,\n",
    ")\n",
    "\n",
    "clip_text_encoder = CLIPTextModel.from_pretrained(\n",
    "    diff_model_name,\n",
    "    subfolder=\"text_encoder\",\n",
    "    revision=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a87461eb-5c13-4424-bc55-af586fb809a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama inference took: 3.5084380309999688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a majestic elephant with a wrinkled gray skin, holding a red apple in its long, curled trunk. the elephant stands in a lush green forest, surrounded by tall trees and a clear blue sky. the sunlight filters through the leaves, casting dappled shadows on the forest floor.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_prompt = get_detailed_caption_with_llama(\"An elephant holding an apple in its trunk\")\n",
    "complex_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81c2b80f-7e33-4c00-a669-44b1c67204de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   320, 15335, 10299,   593,   320, 22201,   912,  7048,  3575,\n",
       "           267,  5050,   320,   736,  3055,   530,   902,  1538,   267, 43734,\n",
       "         18347,   269,   518, 10299,  6446,   530,   320, 16263,  1901,  4167,\n",
       "           267, 13589,   638,  7771,  4682,   537,   320,  3143,  1746,  2390,\n",
       "           269,   518, 17996, 18385,  1417,   518,  5579,   267,  7087, 46857,\n",
       "           912, 12971,   525,   518,  4167,  4125,   269, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = clip_tokenizer(\n",
    "    complex_prompt,\n",
    "    return_tensors='pt',\n",
    "    max_length=clip_tokenizer.model_max_length,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a885b589-b8e2-411b-96e8-76330e633278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 768])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = clip_text_encoder(**tokens)\n",
    "clip_out = embeddings.last_hidden_state\n",
    "clip_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1bdf930c-d66b-49a2-b4f8-abe597bf78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(caption):\n",
    "    complex_prompt = get_detailed_caption_with_llama(caption)\n",
    "    tokens = clip_tokenizer(\n",
    "        complex_prompt,\n",
    "        return_tensors='pt',\n",
    "        max_length=clip_tokenizer.model_max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    embeddings = clip_text_encoder(**tokens)\n",
    "    clip_out = embeddings.last_hidden_state\n",
    "    return clip_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13559b5a-9301-4825-8738-16e815d2ff04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama inference took: 2.828783167002257\n"
     ]
    }
   ],
   "source": [
    "c = inference(\"a racoon holding a red box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "83e98948-1e45-4ce8-9e0e-30667df63284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 768])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5488605c-f4a1-4bd1-a5b7-da81ea95a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SUR_adapter import Adapter\n",
    "suradapter = Adapter(adapter_weight=1e-4, sd_text_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c0f89bcc-13bc-4c62-9bdd-705ae76ddb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 768])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_prompt = \"a racoon holding a red box\"\n",
    "tokens = clip_tokenizer(\n",
    "    simple_prompt,\n",
    "    return_tensors='pt',\n",
    "    max_length=clip_tokenizer.model_max_length,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True\n",
    ")\n",
    "embeddings = clip_text_encoder(**tokens)\n",
    "clip_out = embeddings.last_hidden_state\n",
    "clip_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "84c97783-8b78-459a-b691-be177f0a9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ , _ = suradapter(clip_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d02f7633-615b-4353-9d7b-018225530828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 768])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "38f85f3a-512a-4872-8ed2-fbf46923a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    lf = torch.nn.CosineSimilarity(dim=0, eps=1e-08)\n",
    "    return lf(x.flatten(), y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2eea5f63-1621-4d98-a5c1-49b030ae11f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3128, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(out, c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
